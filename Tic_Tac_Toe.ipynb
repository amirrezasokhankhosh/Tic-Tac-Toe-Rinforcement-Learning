{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic-Tac-Toe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMbgyMdGh7/kYiBE5XWvRrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirrezasokhankhosh/Tic-Tac-Toe-Rinforcement-Learning/blob/main/Tic_Tac_Toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbOCaY-haoA"
      },
      "source": [
        "# Meeting 1 : Formalizing The Problem\n",
        "Environment : Simple Tic-tac-toe game.\\\n",
        "Goal : Winning the game.\\\n",
        "Actions : choosing a grid from unchoosed grids.\\\n",
        "Reward : For each win, draw, loose, the agent will get 1, 0, -1, respectively.\\\n",
        "States : Each grid in gridworld of our problem has 3 possibilities, chose by enemy or us or It's not chosen yet, hense we have 3 ^ 9 states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fge2ct3vkJ-s"
      },
      "source": [
        "# Meeting 2 : Choosing The Learning Algorithm.\n",
        "1. Can we represent the value function with a table? No.\n",
        "2. Are we using average reward? No.\n",
        "3. Will we learn on each timestep? Yes.\n",
        "4. Is this a control problem? Yes.\\\n",
        "So we are now down to choose between 3 algorithms : Expected SARSA, Q Learning and SARSA. We use SARSA in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4wTBZRWmtlk"
      },
      "source": [
        "# Meeting 3 : Agent Architecture Meeting : Overview of Design Choices\n",
        "## Metaparameter Choices\n",
        "### Which Function Approximator Will We Use?\n",
        "Neural Network.\n",
        "#### Which Activation Function Will We Use?\n",
        "\\begin{align}\n",
        "        \\text{f}(x) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        x & x > 0 \\\\\n",
        "        0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "#### How Are We Going to Train The Neural Network?\n",
        "We are going to use the ADAM optimizer.\n",
        "### Which Exploration Method Will We Use?\n",
        "Softmax Policy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P4kUE6CkOJL",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b688b81b-c323-4f8c-caa0-42142cc6e70e"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d6ab8997-665c-4573-aaf6-311068361440\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d6ab8997-665c-4573-aaf6-311068361440\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving agent.py to agent.py\n",
            "Saving environment.py to environment.py\n",
            "User uploaded file \"agent.py\" with length 2041 bytes\n",
            "User uploaded file \"environment.py\" with length 1830 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-1dUIBawL0j"
      },
      "source": [
        "# Required Packages\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from environment import BaseEnvironment\n",
        "from BaseOptimizer import BaseOptimizer\n",
        "from agent import BaseAgent\n",
        "from copy import deepcopy\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFBAoalLcMIp"
      },
      "source": [
        "class Adam(BaseOptimizer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def optimizer_init(self, optimizer_info):\n",
        "        \"\"\"Setup for the optimizer.\n",
        "\n",
        "        Set parameters needed to setup the Adam algorithm.\n",
        "\n",
        "        Assume optimizer_info dict contains:\n",
        "        {\n",
        "            num_states: integer,\n",
        "            num_hidden_layer: integer,\n",
        "            num_hidden_units: integer,\n",
        "            step_size: float,\n",
        "            self.beta_m: float\n",
        "            self.beta_v: float\n",
        "            self.epsilon: float\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_states = optimizer_info.get(\"num_states\")\n",
        "        self.num_hidden_layer = optimizer_info.get(\"num_hidden_layer\")\n",
        "        self.num_hidden_units = optimizer_info.get(\"num_hidden_units\")\n",
        "\n",
        "        # Specify Adam algorithm's hyper parameters\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
        "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
        "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
        "\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize Adam algorithm's m and v\n",
        "        self.m = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        self.v = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            # Initialize self.m[i][\"W\"], self.m[i][\"b\"], self.v[i][\"W\"], self.v[i][\"b\"] to zero\n",
        "            self.m[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i + 1]))\n",
        "            self.m[i][\"b\"] = np.zeros((1, self.layer_size[i + 1]))\n",
        "            self.v[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i + 1]))\n",
        "            self.v[i][\"b\"] = np.zeros((1, self.layer_size[i + 1]))\n",
        "\n",
        "        # Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat\n",
        "        self.beta_m_product = self.beta_m\n",
        "        self.beta_v_product = self.beta_v\n",
        "\n",
        "    def update_weights(self, weights, g):\n",
        "        \"\"\"\n",
        "        Given weights and update g, return updated weights\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "                # update self.m and self.v\n",
        "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * g[i][param]\n",
        "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (g[i][param] * g[i][param])\n",
        "\n",
        "                # compute m_hat and v_hat\n",
        "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
        "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
        "\n",
        "                # update weights\n",
        "                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "        # update self.beta_m_product and self.beta_v_product\n",
        "        self.beta_m_product *= self.beta_m\n",
        "        self.beta_v_product *= self.beta_v\n",
        "\n",
        "        return weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etYffDvqqc3u"
      },
      "source": [
        "# Tic-Tac-Toe Environment\n",
        "\n",
        "class TicTacToeEnvironment(BaseEnvironment):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"\n",
        "        Setup for the environment called when the experiment first starts.\n",
        "        \"\"\"\n",
        "        # initialize board\n",
        "        self.board = np.zeros((3, 3))\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"\n",
        "        The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0.0\n",
        "        self.board = np.zeros((3, 3))\n",
        "        state = np.zeros((3, 3))\n",
        "        actions = self.find_actions(state)\n",
        "        observation = (state, actions)\n",
        "        is_terminal = False\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        # return first state observation from the environment\n",
        "        return self.reward_obs_term[1]\n",
        "\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        self.take_action(action)\n",
        "        self.take_action_opp()\n",
        "        state = np.copy(self.board)\n",
        "        actions = self.find_actions(state)\n",
        "        observation = (state, actions)\n",
        "        reward, is_terminal = self.get_reward()\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        return self.reward_obs_term\n",
        "\n",
        "    def find_actions(self, state):\n",
        "        actions = []\n",
        "        for i in range(0, 3, 1):\n",
        "            for j in range(0, 3, 1):\n",
        "                if state[i, j] == 0:\n",
        "                    actions.append((i, j))\n",
        "        return actions\n",
        "\n",
        "    def get_board(self):\n",
        "        return self.board\n",
        "\n",
        "    def set_board(self, board):\n",
        "        self.board = board\n",
        "\n",
        "    def take_action(self, action):\n",
        "        \"\"\"\n",
        "            0 represents none chosen grid.\n",
        "            1 represents agent action.\n",
        "            2 represents opponent action.\n",
        "        \"\"\"\n",
        "        self.board[int(action[0]), int(action[1])] = 1\n",
        "        \n",
        "    def take_action_opp(self):\n",
        "        possible_actions = self.find_actions(np.copy(self.board))\n",
        "        if len(possible_actions) != 0:\n",
        "            action = random.choice(possible_actions)\n",
        "            self.board[int(action[0]), int(action[1])] = 2\n",
        "\n",
        "    def get_reward(self):\n",
        "        \"\"\"\n",
        "            Win : 10\n",
        "            Draw : 0\n",
        "            Loose : -10\n",
        "            Each time step : -1\n",
        "        \"\"\"\n",
        "        reward = -1\n",
        "        is_terminal = False\n",
        "        if np.count_nonzero(self.board) == 9:\n",
        "            is_terminal = True\n",
        "        for x in self.board:\n",
        "            if np.array_equal(x, np.ones(3)):\n",
        "                reward += 10\n",
        "                is_terminal = True\n",
        "                return reward, is_terminal\n",
        "            elif np.array_equal(x, np.full(3, 2)):\n",
        "                reward += -10\n",
        "                is_terminal = True\n",
        "                return reward, is_terminal\n",
        "        for x in self.board.T:\n",
        "            if np.array_equal(x, np.ones(3)):\n",
        "                reward += 10\n",
        "                is_terminal = True\n",
        "                return reward, is_terminal\n",
        "            elif np.array_equal(x, np.full(3, 2)):\n",
        "                reward += -10\n",
        "                is_terminal = True\n",
        "                return reward, is_terminal\n",
        "        if np.array_equal(np.diagonal(self.board), np.full(3, 1)) or np.array_equal(np.fliplr(self.board).diagonal(),\n",
        "                                                                                    np.full(3, 1)):\n",
        "            reward += 10\n",
        "            is_terminal = True\n",
        "            return reward, is_terminal\n",
        "        elif np.array_equal(np.diagonal(self.board), np.full(3, 2)) or np.array_equal(np.fliplr(self.board).diagonal(),\n",
        "                                                                                      np.full(3, 2)):\n",
        "            reward += -10\n",
        "            is_terminal = True\n",
        "            return reward, is_terminal\n",
        "        return reward, is_terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSbeU6lzSSR"
      },
      "source": [
        "class StateValueNetwork:\n",
        "    def __init__(self, network_config):\n",
        "        # number of states\n",
        "        self.num_states = network_config.get(\"num_states\")\n",
        "        # number of hidden layers\n",
        "        self.num_hidden_layer = network_config.get(\"num_hidden_layer\")\n",
        "        # number of hidden units\n",
        "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
        "        # discount\n",
        "        self.discount = network_config['gamma']\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
        "        \n",
        "        # layer size : NN\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "    \n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            ins, outs = self.layer_size[i], self.layer_size[i + 1]\n",
        "            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (ins, outs))\n",
        "            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (1, outs))\n",
        "    \n",
        "    def get_value(self, s):\n",
        "        \"\"\"\n",
        "        Compute value of input s given the weights of a neural network\n",
        "        \"\"\"\n",
        "        \n",
        "        # DONE!\n",
        "        \n",
        "        psi = self.my_matmul(s, self.weights[0][\"W\"]) + self.weights[0][\"b\"]\n",
        "        x = np.maximum(psi, 0)\n",
        "        v = self.my_matmul(x, self.weights[1][\"W\"]) + self.weights[1][\"b\"]\n",
        "        \n",
        "        return v\n",
        "    \n",
        "    def get_gradient(self, s):\n",
        "        \"\"\"\n",
        "        Given inputs s and weights, return the gradient of v with respect to the weights\n",
        "        \"\"\"\n",
        "        \n",
        "        # DONE!\n",
        "        \n",
        "        grads = [dict() for i in range(len(self.weights))]\n",
        "        x = np.maximum(self.my_matmul(s, self.weights[0][\"W\"]) + self.weights[0][\"b\"], 0)\n",
        "        grads[0][\"W\"] = self.my_matmul(s.T, (self.weights[1][\"W\"].T * (x > 0)))\n",
        "        grads[0][\"b\"] = self.weights[1][\"W\"].T * (x > 0)\n",
        "        grads[1][\"W\"] = x.T\n",
        "        grads[1][\"b\"] = 1\n",
        "\n",
        "        return grads\n",
        "    \n",
        "    \n",
        "    def get_action_values(self, observation):\n",
        "        # find values of each next state\n",
        "        state = observation[0]\n",
        "        possible_actions = observation[1]\n",
        "        \n",
        "        q_values = np.zeros((len(possible_actions), 2))\n",
        "        \n",
        "        for i in range(len(possible_actions)):\n",
        "            temp = state\n",
        "            temp[possible_actions[i][0], possible_actions[i][1]] = 1\n",
        "            state_vec = self.one_hot(self.generate_hash(temp), self.num_states)\n",
        "            v_s = self.get_value(state_vec)\n",
        "            action = 3 * possible_actions[i][0] + possible_actions[i][1]\n",
        "            \n",
        "            # POSSIBLE FAULT    \n",
        "            q_values[i] = (action, -1 + self.discount * v_s)\n",
        "        \n",
        "        return q_values \n",
        "    \n",
        "    \n",
        "    def one_hot(self, state, num_states):\n",
        "        \"\"\"\n",
        "        Given num_state and a state, return the one-hot encoding of the state\n",
        "        \"\"\"\n",
        "\n",
        "        # DONE!\n",
        "        \n",
        "        one_hot_vector = np.zeros((1, num_states))\n",
        "        one_hot_vector[0, int((state - 1))] = 1\n",
        "        \n",
        "        return one_hot_vector\n",
        "    \n",
        "    def generate_hash(self, m):\n",
        "        my_hash = 0\n",
        "        for i in range(0, 3, 1):\n",
        "            for j in range(0, 3, 1):\n",
        "                my_hash = my_hash * 3 + int(m[i][j])\n",
        "        return my_hash\n",
        "\n",
        "    def my_matmul(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Given matrices x1 and x2, return the multiplication of them\n",
        "        \"\"\"\n",
        "        \n",
        "        result = np.zeros((x1.shape[0], x2.shape[1]))\n",
        "        x1_non_zero_indices = x1.nonzero()\n",
        "        if x1.shape[0] == 1 and len(x1_non_zero_indices[1]) == 1:\n",
        "            result = x2[x1_non_zero_indices[1], :]\n",
        "        elif x1.shape[1] == 1 and len(x1_non_zero_indices[0]) == 1:\n",
        "            result[x1_non_zero_indices[0], :] = x2 * x1[x1_non_zero_indices[0], 0]\n",
        "        else:\n",
        "            result = np.matmul(x1, x2)\n",
        "        return result\n",
        "    \n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Returns: \n",
        "            A copy of the current weights of this network.\n",
        "        \"\"\"\n",
        "        return deepcopy(self.weights)\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
        "        \"\"\"\n",
        "        self.weights = deepcopy(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3sJsOkcwuF"
      },
      "source": [
        "class Agent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"expected_sarsa_agent\"\n",
        "\n",
        "    def agent_init(self, agent_config):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the agent.\n",
        "\n",
        "        Assume agent_config dict contains:\n",
        "        {\n",
        "            optimizer_config: dictionary : {\n",
        "                \"num_states\",\n",
        "                \"num_hidden_layer\",\n",
        "                \"num_hidden_units\",\n",
        "                \"step_size\",\n",
        "                \"beta_m\",\n",
        "                \"beta_v\",\n",
        "                \"epsilon\"},\n",
        "            network_config : dictionary : {\n",
        "                \"num_states\",\n",
        "                \"num_hidden_layer\",\n",
        "                \"num_hidden_units\",\n",
        "                \"discount_factor\"\n",
        "            }\n",
        "            discount_factor: float,\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = Adam()\n",
        "        self.optimizer.optimizer_init(agent_config[\"optimizer_config\"])\n",
        "        # network\n",
        "        self.network = StateValueNetwork(agent_config[\"network_config\"])\n",
        "        # discount\n",
        "        self.discount = agent_config['gamma']\n",
        "        # tau\n",
        "        self.tau = agent_config['tau']\n",
        "        # number of states\n",
        "        self.num_states = agent_config.get(\"num_states\")\n",
        "        # number of hidden layers : NN\n",
        "        self.num_hidden_layer = agent_config.get(\"num_hidden_layer\")\n",
        "        # number of hidden units : NN\n",
        "        self.num_hidden_units = agent_config.get(\"num_hidden_units\")\n",
        "\n",
        "        # layer size : NN\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            ins, outs = self.layer_size[i], self.layer_size[i + 1]\n",
        "            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (ins, outs))\n",
        "            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (1, outs))\n",
        "\n",
        "        self.last_observation = None\n",
        "        self.last_action = None\n",
        "\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "\n",
        "    def policy(self, observation):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): the state.\n",
        "        Returns:\n",
        "            the action.\n",
        "        \"\"\"\n",
        "        action_values = self.network.get_action_values(observation)\n",
        "        probs_batch = self.softmax(action_values, self.tau)\n",
        "        try:\n",
        "            index = self.rand_generator.choice(len(observation[1]), p=probs_batch[:, 1].squeeze())\n",
        "        except:\n",
        "            index = 0\n",
        "        action_num = action_values[index, 0]\n",
        "        y = action_num % 3\n",
        "        x = action_num // 3\n",
        "        return (x, y)\n",
        "\n",
        "    def agent_start(self, observation):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "        self.last_observation = (np.copy(observation[0]), observation[1])\n",
        "        self.last_action = self.policy(self.last_observation)\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        # Delta for expected sarsa\n",
        "        last_state_hash = self.network.generate_hash(self.last_observation[0])\n",
        "        last_state_vec = self.network.one_hot(last_state_hash, self.num_states)\n",
        "        last_value = self.network.get_value(last_state_vec)\n",
        "\n",
        "        state_hash = self.network.generate_hash(np.copy(observation[0]))\n",
        "        state_vec = self.network.one_hot(state_hash, self.num_states)\n",
        "        value = self.network.get_value(state_vec)\n",
        "\n",
        "        delta = reward + self.discount * value - last_value\n",
        "\n",
        "        grads = self.network.get_gradient(last_state_vec)\n",
        "\n",
        "        g = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            for param in self.weights[i].keys():\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "\n",
        "        weights = self.optimizer.update_weights(self.weights, g)\n",
        "\n",
        "        self.network.set_weights(weights)\n",
        "\n",
        "        # Select action\n",
        "        action = self.policy((np.copy(observation[0]), observation[1]))\n",
        "\n",
        "        # Update the last state and last action.\n",
        "        self.last_observation = (np.copy(observation[0]), observation[1])\n",
        "        self.last_action = action\n",
        "\n",
        "        return (int(action[0]), int(action[1]))\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        last_state_hash = self.network.generate_hash(self.last_observation[0])\n",
        "        last_state_vec = self.network.one_hot(last_state_hash, self.num_states)\n",
        "        last_value = self.network.get_value(last_state_vec)\n",
        "        delta = reward - last_value\n",
        "\n",
        "        grads = self.network.get_gradient(last_state_vec)\n",
        "\n",
        "        g = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            for param in self.weights[i].keys():\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "\n",
        "        weights = self.optimizer.update_weights(self.weights, g)\n",
        "\n",
        "        self.network.set_weights(weights)\n",
        "\n",
        "    def agent_message(self, message):\n",
        "        if message == \"get_sum_reward\":\n",
        "            return self.sum_rewards\n",
        "        else:\n",
        "            raise Exception(\"Unrecognized Message!\")\n",
        "\n",
        "    def softmax(self, action_values, tau=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action_values (Numpy array): A 2D array of shape (batch_size, num_actions).\n",
        "                        The action-values computed by an action-value network.\n",
        "            tau (float): The temperature parameter scalar.\n",
        "        Returns:\n",
        "            A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
        "            the actions representing the policy.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
        "        preferences = action_values[:, 1] / tau\n",
        "        # Compute the maximum preference across the actions\n",
        "        try: \n",
        "            max_preference = np.max(preferences)\n",
        "        except:\n",
        "            print(action_values)\n",
        "            raise Error\n",
        "\n",
        "        # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting\n",
        "        # when subtracting the maximum preference from the preference of each action.\n",
        "        reshaped_max_preference = max_preference.reshape((-1, 1))\n",
        "\n",
        "        # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
        "        exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
        "        # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
        "        sum_of_exp_preferences = np.sum(exp_preferences)\n",
        "\n",
        "        # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting\n",
        "        # when dividing the numerator by the denominator.\n",
        "        reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
        "\n",
        "        # Compute the action probabilities according to the equation in the previous cell.\n",
        "        probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
        "\n",
        "        # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
        "        # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
        "        # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        action_probs = np.zeros((len(action_values), 2))\n",
        "        for i in range(len(action_probs)):\n",
        "            action_probs[i, 0] = action_values[i, 0]\n",
        "            try:\n",
        "                action_probs[i, 1] = probs[i]\n",
        "            except:\n",
        "                action_probs[i, 1] = 1.0\n",
        "        \n",
        "        return action_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "FzaG2Hc7dpLI",
        "outputId": "0d1d45f0-2187-45f7-fbed-61005239ec2d"
      },
      "source": [
        "# Train\n",
        "\n",
        "agent_config = {\n",
        "    \"optimizer_config\" : {\n",
        "        \"num_states\" : 3 ** 9,\n",
        "        \"num_hidden_layer\" : 1,\n",
        "        \"num_hidden_units\" : 3 ** 6,\n",
        "        \"step_size\" : 3e-5,\n",
        "        \"beta_m\": 0.9,\n",
        "        \"beta_v\": 0.999,\n",
        "        \"epsilon\": 1e-8\n",
        "    },\n",
        "    \"network_config\" : {\n",
        "        \"num_states\" : 3 ** 9,\n",
        "        \"num_hidden_layer\" : 1,\n",
        "        \"num_hidden_units\" : 3 ** 6,\n",
        "        'gamma': 0.99\n",
        "    },\n",
        "    \"num_states\" : 3 ** 9,\n",
        "    \"num_hidden_layer\" : 1,\n",
        "    \"num_hidden_units\" : 3 ** 6,\n",
        "    'replay_buffer_size': 32,\n",
        "    'minibatch_sz': 32,\n",
        "    'num_replay_updates_per_step': 4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 1000.0,\n",
        "    'seed': 0\n",
        "}\n",
        "\n",
        "\n",
        "avg_rewards = np.zeros(10)\n",
        "\n",
        "agent = Agent()\n",
        "agent.agent_init(agent_config)\n",
        "\n",
        "env = TicTacToeEnvironment()\n",
        "env.env_init()\n",
        "for j in range(len(avg_rewards)):\n",
        "    print(\"Progress : \" + str(j * 10) + \"%.\")\n",
        "    rewards = np.zeros(300)\n",
        "    for i in range(len(rewards)):\n",
        "        obs = env.env_start()\n",
        "        action = agent.agent_start(obs)\n",
        "        reward_obs_term = env.env_step(action)\n",
        "        while reward_obs_term[2] != True:\n",
        "            action = agent.agent_step(reward_obs_term[0], reward_obs_term[1])\n",
        "            reward_obs_term = env.env_step(action)\n",
        "        agent.agent_end(reward_obs_term[0])\n",
        "        rewards[i] = agent.sum_rewards\n",
        "    avg_rewards[j] = np.average(rewards)\n",
        "\n",
        "x_axis = np.zeros(10)\n",
        "for i in range(len(x_axis)):\n",
        "    x_axis[i] = i + 1\n",
        "plt.plot(x_axis, avg_rewards)\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress : 0%.\n",
            "Progress : 10%.\n",
            "Progress : 20%.\n",
            "Progress : 30%.\n",
            "Progress : 40%.\n",
            "Progress : 50%.\n",
            "Progress : 60%.\n",
            "Progress : 70%.\n",
            "Progress : 80%.\n",
            "Progress : 90%.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKG965w6lJ2w"
      },
      "source": [
        "Results after 3000 episodes."
      ]
    }
  ]
}