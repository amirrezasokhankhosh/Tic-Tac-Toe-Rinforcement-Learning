{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic-Tac-Toe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6s4U41jw9HQVxmtp0cRye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirrezasokhankhosh/Tic-Tac-Toe-Rinforcement-Learning/blob/main/Tic_Tac_Toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbOCaY-haoA"
      },
      "source": [
        "# Meeting 1 : Formalizing The Problem\n",
        "Environment : Simple Tic-tac-toe game.\\\n",
        "Goal : Winning the game.\\\n",
        "Actions : choosing a grid from unchoosed grids.\\\n",
        "Reward : For each win, draw, loose, the agent will get 1, 0, -1, respectively.\\\n",
        "States : Each grid in gridworld of our problem has 3 possibilities, chose by enemy or us or It's not chosen yet, hense we have 3 ^ 9 states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fge2ct3vkJ-s"
      },
      "source": [
        "# Meeting 2 : Choosing The Learning Algorithm.\n",
        "1. Can we represent the value function with a table? No.\n",
        "2. Are we using average reward? No.\n",
        "3. Will we learn on each timestep? Yes.\n",
        "4. Is this a control problem? Yes.\\\n",
        "So we are now down to choose between 3 algorithms : Expected SARSA, Q Learning and SARSA. We use Expected SARSA in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4wTBZRWmtlk"
      },
      "source": [
        "# Meeting 3 : Agent Architecture Meeting : Overview of Design Choices\n",
        "## Metaparameter Choices\n",
        "### Which Function Approximator Will We Use?\n",
        "Neural Network.\n",
        "#### Which Activation Function Will We Use?\n",
        "\\begin{align}\n",
        "        \\text{f}(x) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        x & x > 0 \\\\\n",
        "        0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "#### How Are We Going to Train The Neural Network?\n",
        "We are going to use the ADAM optimizer.\n",
        "### Which Exploration Method Will We Use?\n",
        "Softmax Policy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P4kUE6CkOJL",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "3d217a4f-d83d-4ae0-8c50-489171a9b7b1"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a38f00a2-7725-4c9a-9718-e777aaef3f6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a38f00a2-7725-4c9a-9718-e777aaef3f6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BaseOptimizer.py to BaseOptimizer.py\n",
            "User uploaded file \"BaseOptimizer.py\" with length 434 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-1dUIBawL0j"
      },
      "source": [
        "# Required Packages\n",
        "\n",
        "import numpy as np\n",
        "from environment import BaseEnvironment\n",
        "from BaseOptimizer import BaseOptimizer\n",
        "from agent import BaseAgent\n",
        "from copy import deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFBAoalLcMIp"
      },
      "source": [
        "class Adam(BaseOptimizer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def optimizer_init(self, optimizer_info):\n",
        "        \"\"\"Setup for the optimizer.\n",
        "\n",
        "        Set parameters needed to setup the Adam algorithm.\n",
        "\n",
        "        Assume optimizer_info dict contains:\n",
        "        {\n",
        "            num_states: integer,\n",
        "            num_hidden_layer: integer,\n",
        "            num_hidden_units: integer,\n",
        "            step_size: float,\n",
        "            self.beta_m: float\n",
        "            self.beta_v: float\n",
        "            self.epsilon: float\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_states = optimizer_info.get(\"num_states\")\n",
        "        self.num_hidden_layer = optimizer_info.get(\"num_hidden_layer\")\n",
        "        self.num_hidden_units = optimizer_info.get(\"num_hidden_units\")\n",
        "\n",
        "        # Specify Adam algorithm's hyper parameters\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
        "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
        "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
        "\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize Adam algorithm's m and v\n",
        "        self.m = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        self.v = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            # Initialize self.m[i][\"W\"], self.m[i][\"b\"], self.v[i][\"W\"], self.v[i][\"b\"] to zero\n",
        "            self.m[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i + 1]))\n",
        "            self.m[i][\"b\"] = np.zeros((1, self.layer_size[i + 1]))\n",
        "            self.v[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i + 1]))\n",
        "            self.v[i][\"b\"] = np.zeros((1, self.layer_size[i + 1]))\n",
        "\n",
        "        # Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat\n",
        "        self.beta_m_product = self.beta_m\n",
        "        self.beta_v_product = self.beta_v\n",
        "\n",
        "    def update_weights(self, weights, g):\n",
        "        \"\"\"\n",
        "        Given weights and update g, return updated weights\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "                # update self.m and self.v\n",
        "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * g[i][param]\n",
        "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (g[i][param] * g[i][param])\n",
        "\n",
        "                # compute m_hat and v_hat\n",
        "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
        "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
        "\n",
        "                # update weights\n",
        "                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "        # update self.beta_m_product and self.beta_v_product\n",
        "        self.beta_m_product *= self.beta_m\n",
        "        self.beta_v_product *= self.beta_v\n",
        "\n",
        "        return weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etYffDvqqc3u"
      },
      "source": [
        "# Tic-Tac-Toe Environment\n",
        "\n",
        "class TicTacToeEnvironment(BaseEnvironment):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"\n",
        "        Setup for the environment called when the experiment first starts.\n",
        "        \"\"\"\n",
        "        # initialize board\n",
        "        self.board = np.zeros((3, 3))\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"\n",
        "        The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0.0\n",
        "        state = np.zeros((3, 3))\n",
        "        actions = self.find_actions(state)\n",
        "        observation = (state, actions)\n",
        "        is_terminal = False\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        # return first state observation from the environment\n",
        "        return self.reward_obs_term[1]\n",
        "\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        self.take_action(action)\n",
        "        self.take_action_opp()\n",
        "        state = np.copy(self.board)\n",
        "        actions = self.find_actions(state)\n",
        "        observation = (state, actions)\n",
        "        reward, is_terminal = self.get_reward()\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        return self.reward_obs_term\n",
        "\n",
        "    def find_actions(self, state):\n",
        "        actions = []\n",
        "        for i in range(0, 3, 1):\n",
        "            for j in range(0, 3, 1):\n",
        "                if state[i, j] == 0:\n",
        "                    actions.append((i, j))\n",
        "        return actions\n",
        "\n",
        "    def get_board(self):\n",
        "        return self.board\n",
        "\n",
        "    def set_board(self, board):\n",
        "        self.board = board\n",
        "\n",
        "    def take_action(self, action):\n",
        "        \"\"\"\n",
        "            0 represents none chosen grid.\n",
        "            1 represents agent action.\n",
        "            2 represents opponent action.\n",
        "        \"\"\"\n",
        "        self.board[int(action[0]), int(action[1])] = 1\n",
        "        \n",
        "    def take_action_opp(self):\n",
        "        possible_actions = self.find_actions(np.copy(self.board))\n",
        "        if len(possible_actions) != 0:\n",
        "            action = random.choice(possible_actions)\n",
        "            self.board[int(action[0]), int(action[1])] = 2\n",
        "\n",
        "    def get_reward(self):\n",
        "        \"\"\"\n",
        "            Win : 10\n",
        "            Draw : 0\n",
        "            Loose : -10\n",
        "            Each time step : -1\n",
        "        \"\"\"\n",
        "        reward = -1\n",
        "        is_terminal = False\n",
        "        if np.count_nonzero(self.board) == 9:\n",
        "            is_terminal = True\n",
        "        for x in self.board:\n",
        "            if np.array_equal(x, np.ones(3)):\n",
        "                reward += 10\n",
        "                is_terminal = True\n",
        "            elif np.array_equal(x, np.full(3, 2)):\n",
        "                reward += -10\n",
        "                is_terminal = True\n",
        "        for x in self.board.T:\n",
        "            if np.array_equal(x, np.ones(3)):\n",
        "                reward += 10\n",
        "                is_terminal = True\n",
        "            elif np.array_equal(x, np.full(3, 2)):\n",
        "                reward += -10\n",
        "                is_terminal = True\n",
        "        if np.array_equal(np.diagonal(self.board), np.full(3, 1)) or np.array_equal(np.fliplr(self.board).diagonal(),\n",
        "                                                                                    np.full(3, 1)):\n",
        "            reward += 10\n",
        "            is_terminal = True\n",
        "        elif np.array_equal(np.diagonal(self.board), np.full(3, 2)) or np.array_equal(np.fliplr(self.board).diagonal(),\n",
        "                                                                                      np.full(3, 2)):\n",
        "            reward += -10\n",
        "            is_terminal = True\n",
        "        return reward, is_terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqVLeJkIq4_b",
        "outputId": "3fce9e89-8ff8-4419-dfc7-c83450da66b1"
      },
      "source": [
        "# Test Environment\n",
        "\n",
        "env = TicTacToeEnvironment()\n",
        "env.env_init()\n",
        "obs = env.env_start()\n",
        "\n",
        "obs = env.env_step((1, 1))\n",
        "\n",
        "obs = env.env_step((0, 1))\n",
        "\n",
        "obs = env.env_step((2, 1))\n",
        "print(obs)\n",
        "del env"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, (array([[0., 1., 0.],\n",
            "       [0., 1., 0.],\n",
            "       [0., 1., 0.]]), [(0, 0), (0, 2), (1, 0), (1, 2), (2, 0), (2, 2)]), True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSbeU6lzSSR"
      },
      "source": [
        "class StateValueNetwork:\n",
        "    def __init__(self, network_config):\n",
        "        # number of states\n",
        "        self.num_states = network_config.get(\"num_states\")\n",
        "        # number of hidden layers\n",
        "        self.num_hidden_layer = network_config.get(\"num_hidden_layer\")\n",
        "        # number of hidden units\n",
        "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
        "        # discount\n",
        "        self.discount = network_config['gamma']\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
        "        \n",
        "        # layer size : NN\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "    \n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            ins, outs = self.layer_size[i], self.layer_size[i + 1]\n",
        "            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (ins, outs))\n",
        "            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (1, outs))\n",
        "    \n",
        "    def get_value(self, s):\n",
        "        \"\"\"\n",
        "        Compute value of input s given the weights of a neural network\n",
        "        \"\"\"\n",
        "        \n",
        "        # DONE!\n",
        "        \n",
        "        psi = self.my_matmul(s, self.weights[0][\"W\"]) + self.weights[0][\"b\"]\n",
        "        x = np.maximum(psi, 0)\n",
        "        v = self.my_matmul(x, self.weights[1][\"W\"]) + self.weights[1][\"b\"]\n",
        "        \n",
        "        return v\n",
        "    \n",
        "    def get_gradient(self, s):\n",
        "        \"\"\"\n",
        "        Given inputs s and weights, return the gradient of v with respect to the weights\n",
        "        \"\"\"\n",
        "        \n",
        "        # DONE!\n",
        "        \n",
        "        grads = [dict() for i in range(len(self.weights))]\n",
        "        x = np.maximum(self.my_matmul(s, self.weights[0][\"W\"]) + self.weights[0][\"b\"], 0)\n",
        "        grads[0][\"W\"] = self.my_matmul(s.T, (self.weights[1][\"W\"].T * (x > 0)))\n",
        "        grads[0][\"b\"] = self.weights[1][\"W\"].T * (x > 0)\n",
        "        grads[1][\"W\"] = x.T\n",
        "        grads[1][\"b\"] = 1\n",
        "\n",
        "        return grads\n",
        "    \n",
        "    \n",
        "    def get_action_values(self, observation):\n",
        "        # find values of each next state\n",
        "        state = observation[0]\n",
        "        possible_actions = observation[1]\n",
        "        \n",
        "        q_values = np.zeros((len(possible_actions), 2))\n",
        "        \n",
        "        for i in range(len(possible_actions)):\n",
        "            temp = state\n",
        "            temp[possible_actions[i][0], possible_actions[i][1]] = 1\n",
        "            state_vec = self.one_hot(self.generate_hash(temp), self.num_states)\n",
        "            v_s = self.get_value(state_vec)\n",
        "            action = 3 * possible_actions[i][0] + possible_actions[i][1]\n",
        "            \n",
        "            # POSSIBLE FAULT    \n",
        "            q_values[i] = (action, -1 + self.discount * v_s)\n",
        "        \n",
        "        return q_values \n",
        "    \n",
        "    \n",
        "    def one_hot(self, state, num_states):\n",
        "        \"\"\"\n",
        "        Given num_state and a state, return the one-hot encoding of the state\n",
        "        \"\"\"\n",
        "\n",
        "        # DONE!\n",
        "        \n",
        "        one_hot_vector = np.zeros((1, num_states))\n",
        "        one_hot_vector[0, int((state - 1))] = 1\n",
        "        \n",
        "        return one_hot_vector\n",
        "    \n",
        "    def generate_hash(self, m):\n",
        "        my_hash = 0\n",
        "        for i in range(0, 3, 1):\n",
        "            for j in range(0, 3, 1):\n",
        "                my_hash = my_hash * 3 + int(m[i][j])\n",
        "        return my_hash\n",
        "\n",
        "    def my_matmul(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Given matrices x1 and x2, return the multiplication of them\n",
        "        \"\"\"\n",
        "        \n",
        "        result = np.zeros((x1.shape[0], x2.shape[1]))\n",
        "        x1_non_zero_indices = x1.nonzero()\n",
        "        if x1.shape[0] == 1 and len(x1_non_zero_indices[1]) == 1:\n",
        "            result = x2[x1_non_zero_indices[1], :]\n",
        "        elif x1.shape[1] == 1 and len(x1_non_zero_indices[0]) == 1:\n",
        "            result[x1_non_zero_indices[0], :] = x2 * x1[x1_non_zero_indices[0], 0]\n",
        "        else:\n",
        "            result = np.matmul(x1, x2)\n",
        "        return result\n",
        "    \n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Returns: \n",
        "            A copy of the current weights of this network.\n",
        "        \"\"\"\n",
        "        return deepcopy(self.weights)\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
        "        \"\"\"\n",
        "        self.weights = deepcopy(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3sJsOkcwuF"
      },
      "source": [
        "class Agent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"expected_sarsa_agent\"\n",
        "\n",
        "    def agent_init(self, agent_config):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the agent.\n",
        "\n",
        "        Assume agent_config dict contains:\n",
        "        {\n",
        "            optimizer_config: dictionary : {\n",
        "                \"num_states\",\n",
        "                \"num_hidden_layer\",\n",
        "                \"num_hidden_units\",\n",
        "                \"step_size\",\n",
        "                \"beta_m\",\n",
        "                \"beta_v\",\n",
        "                \"epsilon\"},\n",
        "            network_config : dictionary : {\n",
        "                \"num_states\",\n",
        "                \"num_hidden_layer\",\n",
        "                \"num_hidden_units\",\n",
        "                \"discount_factor\"\n",
        "            }\n",
        "            discount_factor: float,\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = Adam()\n",
        "        self.optimizer.optimizer_init(agent_config[\"optimizer_config\"])\n",
        "        # network\n",
        "        self.network = StateValueNetwork(agent_config[\"network_config\"])\n",
        "        # discount\n",
        "        self.discount = agent_config['gamma']\n",
        "        # tau\n",
        "        self.tau = agent_config['tau']\n",
        "        # number of states\n",
        "        self.num_states = agent_config.get(\"num_states\")\n",
        "        # number of hidden layers : NN\n",
        "        self.num_hidden_layer = agent_config.get(\"num_hidden_layer\")\n",
        "        # number of hidden units : NN\n",
        "        self.num_hidden_units = agent_config.get(\"num_hidden_units\")\n",
        "\n",
        "        # layer size : NN\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            ins, outs = self.layer_size[i], self.layer_size[i + 1]\n",
        "            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (ins, outs))\n",
        "            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (1, outs))\n",
        "\n",
        "        self.last_observation = None\n",
        "        self.last_action = None\n",
        "\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "\n",
        "    def policy(self, observation):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): the state.\n",
        "        Returns:\n",
        "            the action.\n",
        "        \"\"\"\n",
        "        action_values = self.network.get_action_values(observation)\n",
        "        probs_batch = self.softmax(action_values, self.tau)\n",
        "        try:\n",
        "            index = self.rand_generator.choice(len(observation[1]), p=probs_batch[:, 1].squeeze())\n",
        "        except:\n",
        "            index = 0\n",
        "        action_num = action_values[index, 0]\n",
        "        y = action_num % 3\n",
        "        x = action_num // 3\n",
        "        return (x, y)\n",
        "\n",
        "    def agent_start(self, observation):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "        self.last_observation = (np.copy(observation[0]), observation[1])\n",
        "        self.last_action = self.policy(self.last_observation)\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        # Delta for expected sarsa\n",
        "        last_state_hash = self.network.generate_hash(self.last_observation[0])\n",
        "        last_state_vec = self.network.one_hot(last_state_hash, self.num_states)\n",
        "        last_value = self.network.get_value(last_state_vec)\n",
        "\n",
        "        state_hash = self.network.generate_hash(np.copy(observation[0]))\n",
        "        state_vec = self.network.one_hot(state_hash, self.num_states)\n",
        "        value = self.network.get_value(state_vec)\n",
        "\n",
        "        delta = reward + self.discount * value - last_value\n",
        "\n",
        "        grads = self.network.get_gradient(last_state_vec)\n",
        "\n",
        "        g = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            for param in self.weights[i].keys():\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "\n",
        "        weights = self.optimizer.update_weights(self.weights, g)\n",
        "\n",
        "        self.network.set_weights(weights)\n",
        "\n",
        "        # Select action\n",
        "        action = self.policy((np.copy(observation[0]), observation[1]))\n",
        "\n",
        "        # Update the last state and last action.\n",
        "        self.last_observation = (np.copy(observation[0]), observation[1])\n",
        "        self.last_action = action\n",
        "\n",
        "        return (int(action[0]), int(action[1]))\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        last_state_hash = self.network.generate_hash(self.last_observation[0])\n",
        "        last_state_vec = self.network.one_hot(last_state_hash, self.num_states)\n",
        "        last_value = self.network.get_value(last_state_vec)\n",
        "        delta = reward - last_value\n",
        "\n",
        "        grads = self.network.get_gradient(last_state_vec)\n",
        "\n",
        "        g = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            for param in self.weights[i].keys():\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "\n",
        "        weights = self.optimizer.update_weights(self.weights, g)\n",
        "\n",
        "        self.network.set_weights(weights)\n",
        "\n",
        "    def agent_message(self, message):\n",
        "        if message == \"get_sum_reward\":\n",
        "            return self.sum_rewards\n",
        "        else:\n",
        "            raise Exception(\"Unrecognized Message!\")\n",
        "\n",
        "    def softmax(self, action_values, tau=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action_values (Numpy array): A 2D array of shape (batch_size, num_actions).\n",
        "                        The action-values computed by an action-value network.\n",
        "            tau (float): The temperature parameter scalar.\n",
        "        Returns:\n",
        "            A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
        "            the actions representing the policy.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
        "        preferences = action_values[:, 1] / tau\n",
        "        # Compute the maximum preference across the actions\n",
        "        try: \n",
        "            max_preference = np.max(preferences)\n",
        "        except:\n",
        "            print(action_values)\n",
        "            raise Error\n",
        "\n",
        "        # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting\n",
        "        # when subtracting the maximum preference from the preference of each action.\n",
        "        reshaped_max_preference = max_preference.reshape((-1, 1))\n",
        "\n",
        "        # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
        "        exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
        "        # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
        "        sum_of_exp_preferences = np.sum(exp_preferences)\n",
        "\n",
        "        # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting\n",
        "        # when dividing the numerator by the denominator.\n",
        "        reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
        "\n",
        "        # Compute the action probabilities according to the equation in the previous cell.\n",
        "        probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
        "\n",
        "        # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
        "        # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
        "        # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        action_probs = np.zeros((len(action_values), 2))\n",
        "        for i in range(len(action_probs)):\n",
        "            action_probs[i, 0] = action_values[i, 0]\n",
        "            try:\n",
        "                action_probs[i, 1] = probs[i]\n",
        "            except:\n",
        "                action_probs[i, 1] = 1.0\n",
        "        \n",
        "        return action_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "FzaG2Hc7dpLI",
        "outputId": "4998da75-877a-46d2-cc4f-cc10274f6696"
      },
      "source": [
        "# Train\n",
        "\n",
        "agent_config = {\n",
        "    \"optimizer_config\" : {\n",
        "        \"num_states\" : 3 ** 9,\n",
        "        \"num_hidden_layer\" : 1,\n",
        "        \"num_hidden_units\" : 3 ** 6,\n",
        "        \"step_size\" : 3e-5,\n",
        "        \"beta_m\": 0.9,\n",
        "        \"beta_v\": 0.999,\n",
        "        \"epsilon\": 1e-8\n",
        "    },\n",
        "    \"network_config\" : {\n",
        "        \"num_states\" : 3 ** 9,\n",
        "        \"num_hidden_layer\" : 1,\n",
        "        \"num_hidden_units\" : 3 ** 6,\n",
        "        'gamma': 0.99\n",
        "    },\n",
        "    \"num_states\" : 3 ** 9,\n",
        "    \"num_hidden_layer\" : 1,\n",
        "    \"num_hidden_units\" : 3 ** 6,\n",
        "    'replay_buffer_size': 32,\n",
        "    'minibatch_sz': 32,\n",
        "    'num_replay_updates_per_step': 4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 1000.0,\n",
        "    'seed': 0\n",
        "}\n",
        "\n",
        "rewards = np.zeros(300)\n",
        "\n",
        "agent = Agent()\n",
        "agent.agent_init(agent_config)\n",
        "\n",
        "env = TicTacToeEnvironment()\n",
        "env.env_init()\n",
        "for i in range(len(rewards)):\n",
        "    if i % 30 == 0:\n",
        "        print(\"Progress : \" + str(i / 3) + \"%.\")\n",
        "    obs = env.env_start()\n",
        "    action = agent.agent_start(obs)\n",
        "    reward_obs_term = env.env_step(action)\n",
        "    while reward_obs_term[2] != True:\n",
        "        action = agent.agent_step(reward_obs_term[0], reward_obs_term[1])\n",
        "        reward_obs_term = env.env_step(action)\n",
        "    agent.agent_end(reward_obs_term[0])\n",
        "    rewards[i] = agent.sum_rewards\n",
        "\n",
        "x_axis = np.zeros(300)\n",
        "for i in range(len(x_axis)):\n",
        "    x_axis[i] = i + 1\n",
        "plt.plot(x_axis, rewards)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b219bfc42beb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m }\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    }
  ]
}