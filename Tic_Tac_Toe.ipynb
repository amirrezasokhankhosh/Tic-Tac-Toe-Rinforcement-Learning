{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tic-Tac-Toe.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMswjq834PS1JSaCdvYgtXc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirrezasokhankhosh/Tic-Tac-Toe-Rinforcement-Learning/blob/main/Tic_Tac_Toe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbbOCaY-haoA"
      },
      "source": [
        "# Meeting 1 : Formalizing The Problem\n",
        "Environment : Simple Tic-tac-toe game.\\\n",
        "Goal : Winning the game.\\\n",
        "Actions : choosing a grid from unchoosed grids.\\\n",
        "Reward : For each win, draw, loose, the agent will get 1, 0, -1, respectively.\\\n",
        "States : Each grid in gridworld of our problem has 3 possibilities, chose by enemy or us or It's not chosen yet, hense we have 3 ^ 9 states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fge2ct3vkJ-s"
      },
      "source": [
        "# Meeting 2 : Choosing The Learning Algorithm.\n",
        "1. Can we represent the value function with a table? No.\n",
        "2. Are we using average reward? No.\n",
        "3. Will we learn on each timestep? Yes.\n",
        "4. Is this a control problem? Yes.\\\n",
        "So we are now down to choose between 3 algorithms : Expected SARSA, Q Learning and SARSA. We use SARSA in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4wTBZRWmtlk"
      },
      "source": [
        "# Meeting 3 : Agent Architecture Meeting : Overview of Design Choices\n",
        "## Metaparameter Choices\n",
        "### Which Function Approximator Will We Use?\n",
        "Neural Network.\n",
        "#### Which Activation Function Will We Use?\n",
        "\\begin{align}\n",
        "        \\text{f}(x) = \\left\\{\n",
        "        \\begin{array}{cl}\n",
        "        x & x > 0 \\\\\n",
        "        0\n",
        "        \\end{array}\n",
        "        \\right.\n",
        "    \\end{align}\n",
        "#### How Are We Going to Train The Neural Network?\n",
        "We are going to use the ADAM optimizer.\n",
        "### Which Exploration Method Will We Use?\n",
        "Softmax Policy.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P4kUE6CkOJL",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "ae06a354-9c4f-4782-977e-8e1f5d9b2187"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8c891df7-6fd9-47ce-a327-451f9df12cac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8c891df7-6fd9-47ce-a327-451f9df12cac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BaseOptimizer.py to BaseOptimizer.py\n",
            "User uploaded file \"BaseOptimizer.py\" with length 434 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-1dUIBawL0j"
      },
      "source": [
        "# Required Packages\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from environment import BaseEnvironment\n",
        "from BaseOptimizer import BaseOptimizer\n",
        "from agent import BaseAgent\n",
        "from copy import deepcopy\n",
        "import random"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFBAoalLcMIp"
      },
      "source": [
        "class Adam(BaseOptimizer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def optimizer_init(self, optimizer_info):\n",
        "        \"\"\"Setup for the optimizer.\n",
        "\n",
        "        Set parameters needed to setup the Adam algorithm.\n",
        "\n",
        "        Assume optimizer_info dict contains:\n",
        "        {\n",
        "            num_states: integer,\n",
        "            num_hidden_layer: integer,\n",
        "            num_hidden_units: integer,\n",
        "            step_size: float,\n",
        "            self.beta_m: float\n",
        "            self.beta_v: float\n",
        "            self.epsilon: float\n",
        "        }\n",
        "        \"\"\"\n",
        "\n",
        "        self.num_states = optimizer_info.get(\"num_states\")\n",
        "        self.num_hidden_layer = optimizer_info.get(\"num_hidden_layer\")\n",
        "        self.num_hidden_units = optimizer_info.get(\"num_hidden_units\")\n",
        "\n",
        "        # Specify Adam algorithm's hyper parameters\n",
        "        self.step_size = optimizer_info.get(\"step_size\")\n",
        "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
        "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
        "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
        "\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize Adam algorithm's m and v\n",
        "        self.m = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        self.v = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            # Initialize self.m[i][\"W\"], self.m[i][\"b\"], self.v[i][\"W\"], self.v[i][\"b\"] to zero\n",
        "            self.m[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i + 1]))\n",
        "            self.m[i][\"b\"] = np.zeros((1, self.layer_size[i + 1]))\n",
        "            self.v[i][\"W\"] = np.zeros((self.layer_size[i], self.layer_size[i + 1]))\n",
        "            self.v[i][\"b\"] = np.zeros((1, self.layer_size[i + 1]))\n",
        "\n",
        "        # Initialize beta_m_product and beta_v_product to be later used for computing m_hat and v_hat\n",
        "        self.beta_m_product = self.beta_m\n",
        "        self.beta_v_product = self.beta_v\n",
        "\n",
        "    def update_weights(self, weights, g):\n",
        "        \"\"\"\n",
        "        Given weights and update g, return updated weights\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(len(weights)):\n",
        "            for param in weights[i].keys():\n",
        "                # update self.m and self.v\n",
        "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * g[i][param]\n",
        "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * (g[i][param] * g[i][param])\n",
        "\n",
        "                # compute m_hat and v_hat\n",
        "                m_hat = self.m[i][param] / (1 - self.beta_m_product)\n",
        "                v_hat = self.v[i][param] / (1 - self.beta_v_product)\n",
        "\n",
        "                # update weights\n",
        "                weights[i][param] += self.step_size * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "        # update self.beta_m_product and self.beta_v_product\n",
        "        self.beta_m_product *= self.beta_m\n",
        "        self.beta_v_product *= self.beta_v\n",
        "\n",
        "        return weights\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etYffDvqqc3u"
      },
      "source": [
        "# Tic-Tac-Toe Environment\n",
        "\n",
        "class TicTacToeEnvironment(BaseEnvironment):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def env_init(self, env_info={}):\n",
        "        \"\"\"\n",
        "        Setup for the environment called when the experiment first starts.\n",
        "        \"\"\"\n",
        "        # initialize board\n",
        "        self.board = np.zeros((3, 3))\n",
        "\n",
        "    def env_start(self):\n",
        "        \"\"\"\n",
        "        The first method called when the experiment starts, called before the\n",
        "        agent starts.\n",
        "\n",
        "        Returns:\n",
        "            The first state observation from the environment.\n",
        "        \"\"\"\n",
        "\n",
        "        reward = 0.0\n",
        "        state = np.zeros((3, 3))\n",
        "        actions = self.find_actions(state)\n",
        "        observation = (state, actions)\n",
        "        is_terminal = False\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        # return first state observation from the environment\n",
        "        return self.reward_obs_term[1]\n",
        "\n",
        "    def env_step(self, action):\n",
        "        \"\"\"A step taken by the environment.\n",
        "\n",
        "        Args:\n",
        "            action: The action taken by the agent\n",
        "\n",
        "        Returns:\n",
        "            (float, state, Boolean): a tuple of the reward, state observation,\n",
        "                and boolean indicating if it's terminal.\n",
        "        \"\"\"\n",
        "\n",
        "        self.take_action(action)\n",
        "        self.take_action_opp()\n",
        "        state = np.copy(self.board)\n",
        "        actions = self.find_actions(state)\n",
        "        observation = (state, actions)\n",
        "        reward, is_terminal = self.get_reward()\n",
        "\n",
        "        self.reward_obs_term = (reward, observation, is_terminal)\n",
        "\n",
        "        return self.reward_obs_term\n",
        "\n",
        "    def find_actions(self, state):\n",
        "        actions = []\n",
        "        for i in range(0, 3, 1):\n",
        "            for j in range(0, 3, 1):\n",
        "                if state[i, j] == 0:\n",
        "                    actions.append((i, j))\n",
        "        return actions\n",
        "\n",
        "    def get_board(self):\n",
        "        return self.board\n",
        "\n",
        "    def set_board(self, board):\n",
        "        self.board = board\n",
        "\n",
        "    def take_action(self, action):\n",
        "        \"\"\"\n",
        "            0 represents none chosen grid.\n",
        "            1 represents agent action.\n",
        "            2 represents opponent action.\n",
        "        \"\"\"\n",
        "        self.board[int(action[0]), int(action[1])] = 1\n",
        "        \n",
        "    def take_action_opp(self):\n",
        "        possible_actions = self.find_actions(np.copy(self.board))\n",
        "        if len(possible_actions) != 0:\n",
        "            action = random.choice(possible_actions)\n",
        "            self.board[int(action[0]), int(action[1])] = 2\n",
        "\n",
        "    def get_reward(self):\n",
        "        \"\"\"\n",
        "            Win : 10\n",
        "            Draw : 0\n",
        "            Loose : -10\n",
        "            Each time step : -1\n",
        "        \"\"\"\n",
        "        reward = -1\n",
        "        is_terminal = False\n",
        "        if np.count_nonzero(self.board) == 9:\n",
        "            is_terminal = True\n",
        "        for x in self.board:\n",
        "            if np.array_equal(x, np.ones(3)):\n",
        "                reward += 10\n",
        "                is_terminal = True\n",
        "            elif np.array_equal(x, np.full(3, 2)):\n",
        "                reward += -10\n",
        "                is_terminal = True\n",
        "        for x in self.board.T:\n",
        "            if np.array_equal(x, np.ones(3)):\n",
        "                reward += 10\n",
        "                is_terminal = True\n",
        "            elif np.array_equal(x, np.full(3, 2)):\n",
        "                reward += -10\n",
        "                is_terminal = True\n",
        "        if np.array_equal(np.diagonal(self.board), np.full(3, 1)) or np.array_equal(np.fliplr(self.board).diagonal(),\n",
        "                                                                                    np.full(3, 1)):\n",
        "            reward += 10\n",
        "            is_terminal = True\n",
        "        elif np.array_equal(np.diagonal(self.board), np.full(3, 2)) or np.array_equal(np.fliplr(self.board).diagonal(),\n",
        "                                                                                      np.full(3, 2)):\n",
        "            reward += -10\n",
        "            is_terminal = True\n",
        "        return reward, is_terminal"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wSbeU6lzSSR"
      },
      "source": [
        "class StateValueNetwork:\n",
        "    def __init__(self, network_config):\n",
        "        # number of states\n",
        "        self.num_states = network_config.get(\"num_states\")\n",
        "        # number of hidden layers\n",
        "        self.num_hidden_layer = network_config.get(\"num_hidden_layer\")\n",
        "        # number of hidden units\n",
        "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
        "        # discount\n",
        "        self.discount = network_config['gamma']\n",
        "        \n",
        "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
        "        \n",
        "        # layer size : NN\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "    \n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            ins, outs = self.layer_size[i], self.layer_size[i + 1]\n",
        "            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (ins, outs))\n",
        "            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (1, outs))\n",
        "    \n",
        "    def get_value(self, s):\n",
        "        \"\"\"\n",
        "        Compute value of input s given the weights of a neural network\n",
        "        \"\"\"\n",
        "        \n",
        "        # DONE!\n",
        "        \n",
        "        psi = self.my_matmul(s, self.weights[0][\"W\"]) + self.weights[0][\"b\"]\n",
        "        x = np.maximum(psi, 0)\n",
        "        v = self.my_matmul(x, self.weights[1][\"W\"]) + self.weights[1][\"b\"]\n",
        "        \n",
        "        return v\n",
        "    \n",
        "    def get_gradient(self, s):\n",
        "        \"\"\"\n",
        "        Given inputs s and weights, return the gradient of v with respect to the weights\n",
        "        \"\"\"\n",
        "        \n",
        "        # DONE!\n",
        "        \n",
        "        grads = [dict() for i in range(len(self.weights))]\n",
        "        x = np.maximum(self.my_matmul(s, self.weights[0][\"W\"]) + self.weights[0][\"b\"], 0)\n",
        "        grads[0][\"W\"] = self.my_matmul(s.T, (self.weights[1][\"W\"].T * (x > 0)))\n",
        "        grads[0][\"b\"] = self.weights[1][\"W\"].T * (x > 0)\n",
        "        grads[1][\"W\"] = x.T\n",
        "        grads[1][\"b\"] = 1\n",
        "\n",
        "        return grads\n",
        "    \n",
        "    \n",
        "    def get_action_values(self, observation):\n",
        "        # find values of each next state\n",
        "        state = observation[0]\n",
        "        possible_actions = observation[1]\n",
        "        \n",
        "        q_values = np.zeros((len(possible_actions), 2))\n",
        "        \n",
        "        for i in range(len(possible_actions)):\n",
        "            temp = state\n",
        "            temp[possible_actions[i][0], possible_actions[i][1]] = 1\n",
        "            state_vec = self.one_hot(self.generate_hash(temp), self.num_states)\n",
        "            v_s = self.get_value(state_vec)\n",
        "            action = 3 * possible_actions[i][0] + possible_actions[i][1]\n",
        "            \n",
        "            # POSSIBLE FAULT    \n",
        "            q_values[i] = (action, -1 + self.discount * v_s)\n",
        "        \n",
        "        return q_values \n",
        "    \n",
        "    \n",
        "    def one_hot(self, state, num_states):\n",
        "        \"\"\"\n",
        "        Given num_state and a state, return the one-hot encoding of the state\n",
        "        \"\"\"\n",
        "\n",
        "        # DONE!\n",
        "        \n",
        "        one_hot_vector = np.zeros((1, num_states))\n",
        "        one_hot_vector[0, int((state - 1))] = 1\n",
        "        \n",
        "        return one_hot_vector\n",
        "    \n",
        "    def generate_hash(self, m):\n",
        "        my_hash = 0\n",
        "        for i in range(0, 3, 1):\n",
        "            for j in range(0, 3, 1):\n",
        "                my_hash = my_hash * 3 + int(m[i][j])\n",
        "        return my_hash\n",
        "\n",
        "    def my_matmul(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Given matrices x1 and x2, return the multiplication of them\n",
        "        \"\"\"\n",
        "        \n",
        "        result = np.zeros((x1.shape[0], x2.shape[1]))\n",
        "        x1_non_zero_indices = x1.nonzero()\n",
        "        if x1.shape[0] == 1 and len(x1_non_zero_indices[1]) == 1:\n",
        "            result = x2[x1_non_zero_indices[1], :]\n",
        "        elif x1.shape[1] == 1 and len(x1_non_zero_indices[0]) == 1:\n",
        "            result[x1_non_zero_indices[0], :] = x2 * x1[x1_non_zero_indices[0], 0]\n",
        "        else:\n",
        "            result = np.matmul(x1, x2)\n",
        "        return result\n",
        "    \n",
        "    def get_weights(self):\n",
        "        \"\"\"\n",
        "        Returns: \n",
        "            A copy of the current weights of this network.\n",
        "        \"\"\"\n",
        "        return deepcopy(self.weights)\n",
        "    \n",
        "    def set_weights(self, weights):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
        "        \"\"\"\n",
        "        self.weights = deepcopy(weights)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY3sJsOkcwuF"
      },
      "source": [
        "class Agent(BaseAgent):\n",
        "    def __init__(self):\n",
        "        self.name = \"expected_sarsa_agent\"\n",
        "\n",
        "    def agent_init(self, agent_config):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"Setup for the agent called when the experiment first starts.\n",
        "\n",
        "        Set parameters needed to setup the agent.\n",
        "\n",
        "        Assume agent_config dict contains:\n",
        "        {\n",
        "            optimizer_config: dictionary : {\n",
        "                \"num_states\",\n",
        "                \"num_hidden_layer\",\n",
        "                \"num_hidden_units\",\n",
        "                \"step_size\",\n",
        "                \"beta_m\",\n",
        "                \"beta_v\",\n",
        "                \"epsilon\"},\n",
        "            network_config : dictionary : {\n",
        "                \"num_states\",\n",
        "                \"num_hidden_layer\",\n",
        "                \"num_hidden_units\",\n",
        "                \"discount_factor\"\n",
        "            }\n",
        "            discount_factor: float,\n",
        "        }\n",
        "        \"\"\"\n",
        "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = Adam()\n",
        "        self.optimizer.optimizer_init(agent_config[\"optimizer_config\"])\n",
        "        # network\n",
        "        self.network = StateValueNetwork(agent_config[\"network_config\"])\n",
        "        # discount\n",
        "        self.discount = agent_config['gamma']\n",
        "        # tau\n",
        "        self.tau = agent_config['tau']\n",
        "        # number of states\n",
        "        self.num_states = agent_config.get(\"num_states\")\n",
        "        # number of hidden layers : NN\n",
        "        self.num_hidden_layer = agent_config.get(\"num_hidden_layer\")\n",
        "        # number of hidden units : NN\n",
        "        self.num_hidden_units = agent_config.get(\"num_hidden_units\")\n",
        "\n",
        "        # layer size : NN\n",
        "        self.layer_size = np.array([self.num_states, self.num_hidden_units, 1])\n",
        "\n",
        "        # Initialize the neural network's parameter\n",
        "        self.weights = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            ins, outs = self.layer_size[i], self.layer_size[i + 1]\n",
        "            self.weights[i]['W'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (ins, outs))\n",
        "            self.weights[i]['b'] = self.rand_generator.normal(0, np.sqrt(2 / ins), (1, outs))\n",
        "\n",
        "        self.last_observation = None\n",
        "        self.last_action = None\n",
        "\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "\n",
        "    def policy(self, observation):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            state (Numpy array): the state.\n",
        "        Returns:\n",
        "            the action.\n",
        "        \"\"\"\n",
        "        action_values = self.network.get_action_values(observation)\n",
        "        probs_batch = self.softmax(action_values, self.tau)\n",
        "        try:\n",
        "            index = self.rand_generator.choice(len(observation[1]), p=probs_batch[:, 1].squeeze())\n",
        "        except:\n",
        "            index = 0\n",
        "        action_num = action_values[index, 0]\n",
        "        y = action_num % 3\n",
        "        x = action_num // 3\n",
        "        return (x, y)\n",
        "\n",
        "    def agent_start(self, observation):\n",
        "\n",
        "        # DONE!\n",
        "\n",
        "        \"\"\"The first method called when the experiment starts, called after\n",
        "        the environment starts.\n",
        "        Args:\n",
        "            state (Numpy array): the state from the\n",
        "                environment's evn_start function.\n",
        "        Returns:\n",
        "            The first action the agent takes.\n",
        "        \"\"\"\n",
        "        self.sum_rewards = 0\n",
        "        self.episode_steps = 0\n",
        "        self.last_observation = (np.copy(observation[0]), observation[1])\n",
        "        self.last_action = self.policy(self.last_observation)\n",
        "        return self.last_action\n",
        "\n",
        "    def agent_step(self, reward, observation):\n",
        "        \"\"\"A step taken by the agent.\n",
        "        Args:\n",
        "            reward (float): the reward received for taking the last action taken\n",
        "            state (Numpy array): the state from the\n",
        "                environment's step based, where the agent ended up after the\n",
        "                last step\n",
        "        Returns:\n",
        "            The action the agent is taking.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        # Delta for expected sarsa\n",
        "        last_state_hash = self.network.generate_hash(self.last_observation[0])\n",
        "        last_state_vec = self.network.one_hot(last_state_hash, self.num_states)\n",
        "        last_value = self.network.get_value(last_state_vec)\n",
        "\n",
        "        state_hash = self.network.generate_hash(np.copy(observation[0]))\n",
        "        state_vec = self.network.one_hot(state_hash, self.num_states)\n",
        "        value = self.network.get_value(state_vec)\n",
        "\n",
        "        delta = reward + self.discount * value - last_value\n",
        "\n",
        "        grads = self.network.get_gradient(last_state_vec)\n",
        "\n",
        "        g = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            for param in self.weights[i].keys():\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "\n",
        "        weights = self.optimizer.update_weights(self.weights, g)\n",
        "\n",
        "        self.network.set_weights(weights)\n",
        "\n",
        "        # Select action\n",
        "        action = self.policy((np.copy(observation[0]), observation[1]))\n",
        "\n",
        "        # Update the last state and last action.\n",
        "        self.last_observation = (np.copy(observation[0]), observation[1])\n",
        "        self.last_action = action\n",
        "\n",
        "        return (int(action[0]), int(action[1]))\n",
        "\n",
        "    def agent_end(self, reward):\n",
        "        \"\"\"Run when the agent terminates.\n",
        "        Args:\n",
        "            reward (float): the reward the agent received for entering the\n",
        "                terminal state.\n",
        "        \"\"\"\n",
        "        self.sum_rewards += reward\n",
        "        self.episode_steps += 1\n",
        "\n",
        "        last_state_hash = self.network.generate_hash(self.last_observation[0])\n",
        "        last_state_vec = self.network.one_hot(last_state_hash, self.num_states)\n",
        "        last_value = self.network.get_value(last_state_vec)\n",
        "        delta = reward - last_value\n",
        "\n",
        "        grads = self.network.get_gradient(last_state_vec)\n",
        "\n",
        "        g = [dict() for i in range(self.num_hidden_layer + 1)]\n",
        "        for i in range(self.num_hidden_layer + 1):\n",
        "            for param in self.weights[i].keys():\n",
        "                g[i][param] = delta * grads[i][param]\n",
        "\n",
        "        weights = self.optimizer.update_weights(self.weights, g)\n",
        "\n",
        "        self.network.set_weights(weights)\n",
        "\n",
        "    def agent_message(self, message):\n",
        "        if message == \"get_sum_reward\":\n",
        "            return self.sum_rewards\n",
        "        else:\n",
        "            raise Exception(\"Unrecognized Message!\")\n",
        "\n",
        "    def softmax(self, action_values, tau=1.0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            action_values (Numpy array): A 2D array of shape (batch_size, num_actions).\n",
        "                        The action-values computed by an action-value network.\n",
        "            tau (float): The temperature parameter scalar.\n",
        "        Returns:\n",
        "            A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
        "            the actions representing the policy.\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
        "        preferences = action_values[:, 1] / tau\n",
        "        # Compute the maximum preference across the actions\n",
        "        try: \n",
        "            max_preference = np.max(preferences)\n",
        "        except:\n",
        "            print(action_values)\n",
        "            raise Error\n",
        "\n",
        "        # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting\n",
        "        # when subtracting the maximum preference from the preference of each action.\n",
        "        reshaped_max_preference = max_preference.reshape((-1, 1))\n",
        "\n",
        "        # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
        "        exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
        "        # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
        "        sum_of_exp_preferences = np.sum(exp_preferences)\n",
        "\n",
        "        # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting\n",
        "        # when dividing the numerator by the denominator.\n",
        "        reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
        "\n",
        "        # Compute the action probabilities according to the equation in the previous cell.\n",
        "        probs = exp_preferences / reshaped_sum_of_exp_preferences\n",
        "\n",
        "        # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
        "        # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
        "        # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
        "        probs = probs.squeeze()\n",
        "\n",
        "        action_probs = np.zeros((len(action_values), 2))\n",
        "        for i in range(len(action_probs)):\n",
        "            action_probs[i, 0] = action_values[i, 0]\n",
        "            try:\n",
        "                action_probs[i, 1] = probs[i]\n",
        "            except:\n",
        "                action_probs[i, 1] = 1.0\n",
        "        \n",
        "        return action_probs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "FzaG2Hc7dpLI",
        "outputId": "24dcc79d-4299-44fa-93a7-929067d3198f"
      },
      "source": [
        "# Train\n",
        "\n",
        "agent_config = {\n",
        "    \"optimizer_config\" : {\n",
        "        \"num_states\" : 3 ** 9,\n",
        "        \"num_hidden_layer\" : 1,\n",
        "        \"num_hidden_units\" : 3 ** 6,\n",
        "        \"step_size\" : 3e-5,\n",
        "        \"beta_m\": 0.9,\n",
        "        \"beta_v\": 0.999,\n",
        "        \"epsilon\": 1e-8\n",
        "    },\n",
        "    \"network_config\" : {\n",
        "        \"num_states\" : 3 ** 9,\n",
        "        \"num_hidden_layer\" : 1,\n",
        "        \"num_hidden_units\" : 3 ** 6,\n",
        "        'gamma': 0.99\n",
        "    },\n",
        "    \"num_states\" : 3 ** 9,\n",
        "    \"num_hidden_layer\" : 1,\n",
        "    \"num_hidden_units\" : 3 ** 6,\n",
        "    'replay_buffer_size': 32,\n",
        "    'minibatch_sz': 32,\n",
        "    'num_replay_updates_per_step': 4,\n",
        "    'gamma': 0.99,\n",
        "    'tau': 1000.0,\n",
        "    'seed': 0\n",
        "}\n",
        "\n",
        "rewards = np.zeros(300)\n",
        "\n",
        "agent = Agent()\n",
        "agent.agent_init(agent_config)\n",
        "\n",
        "env = TicTacToeEnvironment()\n",
        "env.env_init()\n",
        "for i in range(len(rewards)):\n",
        "    if i % 30 == 0:\n",
        "        print(\"Progress : \" + str(i / 3) + \"%.\")\n",
        "    obs = env.env_start()\n",
        "    action = agent.agent_start(obs)\n",
        "    reward_obs_term = env.env_step(action)\n",
        "    while reward_obs_term[2] != True:\n",
        "        action = agent.agent_step(reward_obs_term[0], reward_obs_term[1])\n",
        "        reward_obs_term = env.env_step(action)\n",
        "    agent.agent_end(reward_obs_term[0])\n",
        "    rewards[i] = agent.sum_rewards\n",
        "\n",
        "x_axis = np.zeros(300)\n",
        "for i in range(len(x_axis)):\n",
        "    x_axis[i] = i + 1\n",
        "plt.plot(x_axis, rewards)\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress : 0.0%.\n",
            "Progress : 10.0%.\n",
            "Progress : 20.0%.\n",
            "Progress : 30.0%.\n",
            "Progress : 40.0%.\n",
            "Progress : 50.0%.\n",
            "Progress : 60.0%.\n",
            "Progress : 70.0%.\n",
            "Progress : 80.0%.\n",
            "Progress : 90.0%.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAD4CAYAAAAJmJb0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAS8klEQVR4nO3dfaxkdX3H8fd378PyWB5kpZSlXUCsYhWhG6LVYCJWAY3QRg1t09KWhNqq0T5ZKE2jaU1rba02tRoqGmqISFEDtbYVEWPbBHSRZxBYQQIU2EstT4vsPH37x5zZPSz37sLM3J2Z33m/ks2dOWfu/f1+c3Y/+5vv+d1zIjORJJVpzaQ7IElaPYa8JBXMkJekghnyklQwQ16SCjY/6Q7UHXLIIblhw4ZJd0OSZsp11133SGauW27fVIX8hg0b2LRp06S7IUkzJSLuXWmf5RpJKpghL0kFM+QlqWCGvCQVzJCXpIIZ8pJUMENekgo2VevkZ8m37lxi0w9+OOluSCrExg0Hc9KLl/19ppEY8kP683+9jTsffpKISfdEUgne+bqjDflpsq3T44xX/gQfO/P4SXdFklZkTX5I7U6PhTnfPknTzZQaUqubLMz79kmabqbUkNrdHovO5CVNOVNqSO1uj4U5z7pKmm6G/JD6Ie/bJ2m6mVJDyEza3TTkJU09U2oI7W4CsOiJV0lTzpQaQrvbA7AmL2nqGfJD2BHyvn2SppspNYSWIS9pRphSQ9hekzfkJU05U2oIncFMft6avKTpZsgPwZq8pFlhSg2h1emXa+bX+PZJmm6m1BAGM/lFyzWSptxYQj4iDoyIyyLiexFxe0S8OiIOjogrI+Ku6utB42hrGliukTQrxpVSHwf+PTNfAhwH3A6cC1yVmccAV1XPi+ASSkmzYuSUiogDgJOACwEys5WZjwKnAxdVL7sIOGPUtqbFYAmlIS9p2o0jpY4EloDPRsT1EfHpiNgXODQzH6xe8xBw6HLfHBHnRMSmiNi0tLQ0hu6svnanqskb8pKm3DhSah44AfhkZh4PbGWn0kxmJpDLfXNmXpCZGzNz47p147+J7Wpou05e0owYR8jfD9yfmddWzy+jH/oPR8RhANXXLWNoaypYk5c0K0ZOqcx8CLgvIn662nQycBtwBXBWte0s4PJR25oWXtZA0qyYH9PPeQ9wcUQsAncDv0H/P5BLI+Js4F7gHWNqa+JcQilpVowl5DPzBmDjMrtOHsfPnzZeT17SrHAqOoRWZ3Di1bdP0nQzpYZgTV7SrDClhjAo18yvsVwjaboZ8kNod3tEwJwhL2nKGfJDaHV7LMytIcKQlzTdDPkhtDtpPV7STDCphtDu9lw+KWkmGPJD6PR6/iKUpJlgUg2h1UlDXtJMMKmG0O72WPQXoSTNAJNqCNbkJc0KQ34I7a41eUmzwaQaQqtrTV7SbDCphtDu9FwnL2kmmFRDaHd73vpP0kww5IdgTV7SrDCphtDqJvNrfOskTb9x3f6vGJ/973u455Gtu3zN/zz6I448ZJ891CNJGp4hX9Pu9vjgv9zGXgtr2HthbsXXrQk44ScP2oM9k6ThGPI1g5uB/O4bXsxvve7oCfdGkkZnYblmcFs/T6pKKoVpVjOYyXuDbkmlMM1qBiG/6HVpJBXCkK9pdyzXSCqLaVbTGpRrDHlJhTDNarbX5C3XSCqEIV/TdiYvqTCmWY0hL6k0pllNyxOvkgpjmtVsX0LpZYQlFcKQr7FcI6k0plmNIS+pNKZZTctr10gqjGlW09l+WQPfFkllMM1qdlygzBOvksowtpCPiLmIuD4ivlI9PzIiro2IzRHxhYhYHFdbq8VyjaTSjDPN3gvcXnv+YeBvM/NFwP8BZ4+xrVXR7njiVVJZxpJmEbEeeDPw6ep5AK8HLqtechFwxjjaWk1ta/KSCjOuNPsY8H6gVz1/AfBoZnaq5/cDhy/3jRFxTkRsiohNS0tLY+rOcAYhP+8FyiQVYuSQj4i3AFsy87phvj8zL8jMjZm5cd26daN2ZySDmvz8GkNeUhnGcSPv1wBvjYjTgL2AHwM+DhwYEfPVbH498MAY2lpV7W6Pxbk19KtNkjT7Rp7JZ+Z5mbk+MzcAZwLfyMxfAa4G3la97Czg8lHbWm3tTs9ryUsqymqeYfwj4PciYjP9Gv2Fq9jWWLS7PW/iLako4yjXbJeZ3wS+WT2+GzhxnD9/tbW66fJJSUUx0WoGNXlJKoWJVtPuWpOXVBZDvqYf8r4lksphotW0rclLKoyJVuPqGkmlMdFq+iderclLKochX9PuWK6RVBYTrabliVdJhTHRalxCKak0hnyNSygllcZEq3EJpaTSmGg1rY4zeUllMdFq2t0ei/PW5CWVw5CvsSYvqTQmWo01eUmlMdFqXCcvqTQmWo2XNZBUmrHeGWqatTo9/uwrt/Hoj9rL7s9MMnEmL6kojQn5zVue5HPX3MsL91/LfmuXH/YxL9yPn91w0B7umSStnsaEfLvbA+AvfvHlnPzSQyfcG0naMxpTmxiEvOUYSU3SmMRrGfKSGqgxidfuJoC/0SqpUZoT8h1n8pKapzGJZ01eUhM1JvHavX65xpCX1CSNSbxBuWbRkJfUII1JvO3lGk+8SmqQ5oW8M3lJDdKYxGt1rclLap7GJN5gJm9NXlKTNCbxBide572UsKQGaU7IVzP5+TWGvKTmaEzIt7rJ4twaIgx5Sc3RmJDv36TbgJfULCOHfEQcERFXR8RtEXFrRLy32n5wRFwZEXdVXyd6N452t8fCfGP+T5MkYDwz+Q7w+5l5LPAq4F0RcSxwLnBVZh4DXFU9n5i2N+mW1EAjp15mPpiZ360ePwHcDhwOnA5cVL3sIuCMUdsaRauTLp+U1DhjTb2I2AAcD1wLHJqZD1a7HgKWvedeRJwTEZsiYtPS0tI4u/MM1uQlNdHYQj4i9gO+CLwvMx+v78vMBHK578vMCzJzY2ZuXLdu3bi68yydnuUaSc0zltSLiAX6AX9xZn6p2vxwRBxW7T8M2DKOtobV6qQhL6lxxrG6JoALgdsz86O1XVcAZ1WPzwIuH7WtUbi6RlITzY/hZ7wG+FXg5oi4odr2x8BfApdGxNnAvcA7xtDW0NrdHovW5CU1zMghn5n/BayUnieP+vPHxSWUkpqoManX6ibzhrykhmlM6rU7lmskNU9zQt5yjaQGakzqGfKSmqgxqdfuuk5eUvM0JvVa3R6L89bkJTVLY0Leco2kJmpM6rU7hryk5mlM6lmTl9REjUi9zKTdc528pOZpRMh3e0kmzuQlNU4jUq/d7V/K3qtQSmqaRqReq9sDnMlLap5GpF67Cnlr8pKaplEh71UoJTVNI1Kv3alq8oa8pIZpROrtqMlbrpHULOO4/d/UevzpNud+8SYeeuxpABadyUtqmKJT79YHHuerNz/EE093+LmjX8DL1x8w6S5J0h5V9Ez+R+0OAH/99uM47ogDJ9wbSdrzip7Jb93WBWDftXMT7okkTUbRIf9Uqz+T32ex6A8skrSiokN++0zekJfUUEWH/GAmv/ei5RpJzVR0yG9tdVmYCxa9MJmkhio6/Z7a1rEeL6nRyg75Vpd9LdVIarDiQ36ftc7kJTVX0SG/tdVxJi+p0YoO+ae2da3JS2q0okN+a6vDPs7kJTVY0SFvTV5S0xUd8lu3WZOX1GxFh/xTLWvykpqt2JDPTJ5qdbwCpaRGW/WQj4hTIuKOiNgcEeeudnsD2zo9eukVKCU126qGfETMAZ8ATgWOBX4pIo5dzTYHtm7rX5zMmbykJlvtmfyJwObMvDszW8AlwOmr3CbQr8cD7L1gyEtqrtUO+cOB+2rP76+2bRcR50TEpojYtLS0NLaGt7YGM3nLNZKaa+IJmJkXABcAbNy4MUf5WQ899jR/eNmNPN3u8mR1wxB/GUpSk632TP4B4Ija8/XVtlVx4/2P8p93PUKr0+OgfRZ4w0sP5eWHH7BazUnS1Fvtmfx3gGMi4kj64X4m8Mur1Vi72wPgI28/jhcfuv9qNSNJM2NVQz4zOxHxbuA/gDngM5l562q11+n2qz0Lc8Uu/5ek52XVa/KZ+VXgq6vdDkCrmskvzMWeaE6Spl5RU95BuWbRmbwkAaWFfGcwky9qWJI0tKLSsD2oyc8XNSxJGlpRaWhNXpKeqaiQH9TkF9YUNSxJGlpRadju9phbE6xZ40xekqC4kE9LNZJUU1TItzo9V9ZIUk1Ridju9lwjL0k1RSViu+tMXpLqikrEdjdZmLcmL0kDhYW8M3lJqisqEa3JS9IzFZWI/SWURQ1JkkZSVCL2yzXW5CVpoKiQd528JD1TUYnY7vZY9AqUkrRdUYloTV6SnqmoRGx3e8x7cTJJ2q6okG91e94wRJJqikpE18lL0jMVlYjtjpcalqS6skLeyxpI0jPMT7oD4/DYU22+/8iTPLGtY8hLUk0RIf+tu5Z4z+evB3CdvCTVFJGIB+y9sP2xNXlJ2qHAkC9iSJI0FkUkoiEvScsrIhHrIe86eUnaoYhE/DFr8pK0rCJCfq52vRovayBJOxSXiAtrihuSJA2tmETcZ3EOgIV5yzWSNFBeyHviVZK2KyYR91ow5CVpZyMlYkR8JCK+FxE3RcSXI+LA2r7zImJzRNwREW8avau7tu9i/woNLqGUpB1GTcQrgZ/JzFcAdwLnAUTEscCZwMuAU4B/iIi5Edvapb0t10jSs4yUiJn5tczsVE+vAdZXj08HLsnMbZl5D7AZOHGUtnZnUJNPcjWbkaSZMs5p728C/1Y9Phy4r7bv/mrbs0TEORGxKSI2LS0tDd34IOSfanWH/hmSVJrdXmo4Ir4O/Pgyu87PzMur15wPdICLn28HMvMC4AKAjRs3Dj0NP2ifRQA6XWfykjSw25DPzDfsan9E/DrwFuDkzBwk7APAEbWXra+2rZo/efOxHLzfIm982aGr2YwkzZRRV9ecArwfeGtmPlXbdQVwZkSsjYgjgWOAb4/S1u4csM8C5536Uk+8SlLNqHeG+ntgLXBlRABck5nvzMxbI+JS4Db6ZZx3ZabFcknaw0YK+cx80S72fQj40Cg/X5I0GmsbklQwQ16SCmbIS1LBDHlJKpghL0kFM+QlqWCx45dUJy8iloB7h/jWQ4BHxtydSXEs08mxTCfH0vdTmbluuR1TFfLDiohNmblx0v0YB8cynRzLdHIsu2e5RpIKZshLUsFKCfkLJt2BMXIs08mxTCfHshtF1OQlScsrZSYvSVqGIS9JBZv5kI+IUyLijojYHBHnTro/z1dE/CAibo6IGyJiU7Xt4Ii4MiLuqr4eNOl+LiciPhMRWyLiltq2ZfsefX9XHaebIuKEyfX82VYYywci4oHq2NwQEafV9p1XjeWOiHjTZHr9bBFxRERcHRG3RcStEfHeavvMHZddjGUWj8teEfHtiLixGssHq+1HRsS1VZ+/EBGL1fa11fPN1f4NQzeemTP7B5gDvg8cBSwCNwLHTrpfz3MMPwAO2WnbXwHnVo/PBT486X6u0PeTgBOAW3bXd+A0+jd6D+BVwLWT7v9zGMsHgD9Y5rXHVn/X1gJHVn8H5yY9hqpvhwEnVI/3B+6s+jtzx2UXY5nF4xLAftXjBeDa6v2+FDiz2v4p4Lerx78DfKp6fCbwhWHbnvWZ/InA5sy8OzNbwCXA6RPu0zicDlxUPb4IOGOCfVlRZn4L+OFOm1fq++nAP2XfNcCBEXHYnunp7q0wlpWcDlySmdsy8x5gM/2/ixOXmQ9m5nerx08AtwOHM4PHZRdjWck0H5fMzCerpwvVnwReD1xWbd/5uAyO12XAyVHdfu/5mvWQPxy4r/b8fnb9l2AaJfC1iLguIs6pth2amQ9Wjx8CZunu5Cv1fVaP1burMsZnamWzmRhL9RH/ePqzxpk+LjuNBWbwuETEXETcAGwBrqT/SePRzOxUL6n3d/tYqv2PAS8Ypt1ZD/kSvDYzTwBOBd4VESfVd2b/89pMrnOd5b5XPgkcDbwSeBD4m8l257mLiP2ALwLvy8zH6/tm7bgsM5aZPC6Z2c3MVwLr6X/CeMmeaHfWQ/4B4Ija8/XVtpmRmQ9UX7cAX6Z/8B8efGSuvm6ZXA+ft5X6PnPHKjMfrv5h9oB/ZMdH/6keS0Qs0A/FizPzS9XmmTwuy41lVo/LQGY+ClwNvJp+eWxwr+16f7ePpdp/APC/w7Q36yH/HeCY6gz1Iv0TFFdMuE/PWUTsGxH7Dx4DbwRuoT+Gs6qXnQVcPpkeDmWlvl8B/Fq1muNVwGO18sFU2qk2/Qv0jw30x3JmtQLiSOAY4Nt7un/Lqeq2FwK3Z+ZHa7tm7risNJYZPS7rIuLA6vHewM/TP8dwNfC26mU7H5fB8Xob8I3qE9jzN+mzzmM4a30a/bPu3wfOn3R/nmffj6K/GuBG4NZB/+nX3q4C7gK+Dhw86b6u0P/P0/+43KZfTzx7pb7TX13wieo43QxsnHT/n8NYPlf19abqH91htdefX43lDuDUSfe/1q/X0i/F3ATcUP05bRaPyy7GMovH5RXA9VWfbwH+tNp+FP3/iDYD/wysrbbvVT3fXO0/ati2vayBJBVs1ss1kqRdMOQlqWCGvCQVzJCXpIIZ8pJUMENekgpmyEtSwf4fBzLnToDA48oAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBi_Kdm37IPa"
      },
      "source": [
        "As it's obvious from agent's rewards, the agent is learning successfully."
      ]
    }
  ]
}